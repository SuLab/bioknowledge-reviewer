{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @name graph_v3.2_v20190616\n",
    "# @description notebook to build the NGLY1 Deficiency review knowledge graph v3.2\n",
    "# @author NÃºria Queralt Rosinach\n",
    "# @date 16 June 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Description\n",
    "\n",
    "This is the notebook for the creation of the first review network and derived hypotheses. \n",
    "\n",
    "* Using intermediary variables from workflow objects. In this workflow variables are directly used for the next step. \n",
    "\n",
    "\n",
    "* Review network: From Monarch knowledge graph, we built a network seeded by 8 nodes, retrieving their explicit relationships and all the relationships among all these nodes. Seed nodes:\n",
    "\n",
    "    - 'MONDO:0014109', # NGLY1 deficiency\n",
    "    - 'HGNC:17646', # NGLY1 human gene\n",
    "    - 'HGNC:633', # AQP1 human gene\n",
    "    - 'MGI:103201', # AQP1 mouse gene\n",
    "    - 'HGNC:7781', # NFE2L1 human gene\n",
    "    - 'HGNC:24622', # ENGASE human gene\n",
    "    - 'HGNC:636', # AQP3 human gene\n",
    "    - 'HGNC:19940' # AQP11 human gene\n",
    "    \n",
    "\n",
    "* Connecting paths: query templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transcriptomics, regulation, curation, monarch, graph, neo4jlib, hypothesis, summary, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges library\n",
    "### Review edges to integrate into the knowledge graph and prepare them as individual networks\n",
    "\n",
    "#### TRANSCRIPTOMICS NETWORK\n",
    "#### import transcriptomics\n",
    "We retrieved edges from RNA-seq transcriptomics profiles using the `transcriptomics` module:\n",
    "\n",
    "    - Experimental data sets: from Chow et al. paper [pmid:29346549] (NGLY1 deficiency model on fruit fly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"read_data()\" is running...\n",
      "\n",
      "* This is the size of the raw expression data structure: (15370, 9)\n",
      "* These are the expression attributes: Index(['FlyBase ID', 'Symbol', 'baseMean', 'log2FoldChange', 'Unnamed: 4',\n",
      "       'lfcSE', 'stat', 'pvalue', 'padj'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "    FlyBase ID  Symbol    baseMean  log2FoldChange  Unnamed: 4     lfcSE  \\\n",
      "0  FBgn0030880  CG6788  175.577087       -4.209283     0.05406  0.190308   \n",
      "\n",
      "        stat         pvalue           padj  \n",
      "0 -22.118249  2.110000e-108  2.860000e-104  \n",
      "\n",
      "The raw data is saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/transcriptomics/ngly1-fly-chow-2018/data/supp_table_1.csv\n",
      "\n",
      "\n",
      "Finished read_data().\n",
      "\n",
      "\n",
      "The function \"clean_data()\" is running. Keeping only data with FC > 1.5 and FDR < 5% ...\n",
      "\n",
      "* This is the size of the clean expression data structure: (386, 6)\n",
      "* These are the clean expression attributes: Index(['FlyBase ID', 'Symbol', 'log2FoldChange', 'pvalue', 'padj',\n",
      "       'Regulation'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "    FlyBase ID Symbol  log2FoldChange        pvalue      padj   Regulation\n",
      "0  FBgn0035904  GstO3        0.576871  2.130000e-08  0.000002  Upregulated\n",
      "\n",
      "The clean data is saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/transcriptomics/ngly1-fly-chow-2018/out/fc1.5_fdr5_transcriptome_fly.csv\n",
      "\n",
      "\n",
      "Finished clean_data().\n",
      "\n",
      "\n",
      "The function \"prepare_data_edges()\" is running...\n",
      "\n",
      "* This is the size of the expression data structure: (386, 13)\n",
      "* These are the expression attributes: Index(['flybase_id', 'symbol', 'log2FoldChange', 'pvalue', 'padj',\n",
      "       'regulation', 'source', 'subject_id', 'subject_label', 'property_id',\n",
      "       'property_label', 'reference_id', 'object_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "    flybase_id symbol  log2FoldChange        pvalue      padj   regulation  \\\n",
      "0  FBgn0035904  GstO3        0.576871  2.130000e-08  0.000002  Upregulated   \n",
      "\n",
      "  source           subject_id subject_label property_id  property_label  \\\n",
      "0   Chow  FlyBase:FBgn0033050          Pngl  RO:0002434  interacts with   \n",
      "\n",
      "    reference_id            object_id  \n",
      "0  PMID:29346549  FlyBase:FBgn0035904  \n",
      "\n",
      "The ngly1-fly-chow-2018 transcriptomics expression edges are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/transcriptomics/ngly1-fly-chow-2018/out/chow_fc1.5_fdr5_transcriptome_fly_edges.csv\n",
      "\n",
      "\n",
      "Finished prepare_data_edges().\n",
      "\n",
      "\n",
      "The function \"prepare_rna_edges()\" is running...\n",
      "\n",
      "* This is the size of the edges data structure: (386, 12)\n",
      "* These are the edges attributes: Index(['subject_id', 'subject_label', 'property_id', 'property_label',\n",
      "       'object_id', 'object_label', 'log2FoldChange', 'pvalue', 'fdr',\n",
      "       'regulation', 'source', 'reference_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "            subject_id subject_label property_id  property_label  \\\n",
      "0  FlyBase:FBgn0033050          Pngl  RO:0002434  interacts with   \n",
      "\n",
      "             object_id object_label  log2FoldChange        pvalue       fdr  \\\n",
      "0  FlyBase:FBgn0035904        GstO3        0.576871  2.130000e-08  0.000002   \n",
      "\n",
      "    regulation source   reference_id  \n",
      "0  Upregulated   Chow  PMID:29346549  \n",
      "\n",
      "This data object is not saved.\n",
      "\n",
      "\n",
      "Finished prepare_rna_edges().\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "\n",
      "* This is the size of the edges file data structure: (386, 9)\n",
      "* These are the edges attributes: Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "             object_id property_description property_id  property_label  \\\n",
      "0  FlyBase:FBgn0035904                   NA  RO:0002434  interacts with   \n",
      "\n",
      "                                property_uri reference_date  \\\n",
      "0  http://purl.obolibrary.org/obo/RO_0002434     2018-03-15   \n",
      "\n",
      "                           reference_supporting_text  \\\n",
      "0  To understand how loss of NGLY1 contributes to...   \n",
      "\n",
      "                                  reference_uri           subject_id  \n",
      "0  https://www.ncbi.nlm.nih.gov/pubmed/29346549  FlyBase:FBgn0033050  \n",
      "\n",
      "The transcriptomics network edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/rna_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodes()\" is running...\n",
      "* Total number of nodes: 386\n",
      "querying 1-386...done.\n",
      "Finished.\n",
      "\n",
      "* This is the size of the nodes file data structure: (386, 6)\n",
      "* These are the nodes attributes: Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "  description                   id         name preflabel semantic_groups  \\\n",
      "0         NaN  FlyBase:FBgn0033050  PNGase-like      Pngl            GENE   \n",
      "\n",
      "                                           synonyms  \n",
      "0  CG7865|Dmel\\CG7865|PNGase|dNGLY1|ngly1|png1|pngl  \n",
      "\n",
      "The transcriptomics network nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/rna_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "CPU times: user 424 ms, sys: 21.8 ms, total: 446 ms\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare data to graph schema\n",
    "csv_path = './transcriptomics/ngly1-fly-chow-2018/data/supp_table_1.csv'\n",
    "data = transcriptomics.read_data(csv_path)\n",
    "clean_data = transcriptomics.clean_data(data)\n",
    "data_edges = transcriptomics.prepare_data_edges(clean_data)\n",
    "rna_network = transcriptomics.prepare_rna_edges(data_edges)\n",
    "\n",
    "# build network with graph schema\n",
    "rna_edges = transcriptomics.build_edges(rna_network)\n",
    "rna_nodes = transcriptomics.build_nodes(rna_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transcriptomics network is returned as both digital object (`rna_edges`, `rna_nodes`) and CSV files at _**graph/**_ (`rna_edges_version.csv`, `rna_nodes_version.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'list'>\n",
      "type nodes: <class 'list'>\n",
      "\n",
      "len edges: 386\n",
      "len nodes: 386\n",
      "\n",
      "attribute edges: dict_keys(['subject_id', 'object_id', 'property_id', 'property_label', 'property_description', 'property_uri', 'reference_uri', 'reference_supporting_text', 'reference_date'])\n",
      "attribute nodes: dict_keys(['id', 'semantic_groups', 'preflabel', 'name', 'synonyms', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(rna_edges))\n",
    "print('type nodes:', type(rna_nodes))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(rna_edges))\n",
    "print('len nodes:', len(rna_nodes))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', rna_edges[0].keys())\n",
    "print('attribute nodes:', rna_nodes[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULATION NETWORK\n",
    "#### import regulation\n",
    "\n",
    "We retrieved human TF gene expression regulation edges from several sources using the `regulation` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"prepare_msigdb_data()\" is running...\n",
      "\n",
      "* Number of Transcription Factor Targets (TFT) gene sets: 615\n",
      "\n",
      "The MSigDB raw network is saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/regulation/msigdb/out/tf_genelist_entrez_msigdb.json. Other reporting files are also saved at the same directory.\n",
      "\n",
      "\n",
      "Finished prepare_msigdb_data().\n",
      "\n",
      "\n",
      "The function \"load_tf_gene_edges()\" is running...\n",
      "\n",
      "Finished load_tf_gene_edges().\n",
      "\n",
      "\n",
      "The function \"get_gene_id_normalization_dictionaries()\" is running...\n",
      "\n",
      "* Querying BioThings to map gene symbols to HGNC and Entrez IDs...\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-3071...done.\n",
      "Finished.\n",
      "53 input query terms found no hit:\n",
      "\t['MEIS1AHOXA9', 'ALPHACP1', 'HMEF2', 'CEBPDELTA', 'E2F4DP1', 'MMEF2', 'AMEF2', 'TAL1ALPHAE47', 'CACB\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "Saving not found gene symbols at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/regulation/not_found_symbols.list\n",
      "\n",
      "\n",
      "* Querying BioThings to map Entrez to HGNC IDs and gene symbols...\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "querying 10001-11000...done.\n",
      "querying 11001-12000...done.\n",
      "querying 12001-13000...done.\n",
      "querying 13001-14000...done.\n",
      "querying 14001-15000...done.\n",
      "querying 15001-16000...done.\n",
      "querying 16001-16632...done.\n",
      "Finished.\n",
      "140 input query terms found no hit:\n",
      "\t['401157', '170951', '84796', '55265', '284083', '283476', '143914', '283328', '54073', '114299', '7\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "Saving not found Entrez gene IDs at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/regulation/not_found_entrez.list\n",
      "\n",
      "\n",
      "Finished get_gene_id_normalization_dictionaries().\n",
      "\n",
      "\n",
      "The function \"prepare_data_edges()\" is running...\n",
      "\n",
      "The tftargets edges are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/regulation/tftargets/tftargets_edges.csv\n",
      "\n",
      "\n",
      "The MSigDB edges are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/regulation/msigdb/msigdb_edges.csv\n",
      "\n",
      "\n",
      "Finished prepare_data_edges().\n",
      "\n",
      "\n",
      "The function \"prepare_regulation_edges()\" is running...\n",
      "\n",
      "Finished prepare_regulation_edges().\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "\n",
      "* This is the size of the edges file data structure: (197267, 9)\n",
      "* These are the edges attributes: Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "   object_id property_description property_id  property_label  \\\n",
      "0  HGNC:8803                   NA  RO:0002434  interacts with   \n",
      "\n",
      "                                property_uri reference_date  \\\n",
      "0  http://purl.obolibrary.org/obo/RO_0002434     2007-01-01   \n",
      "\n",
      "                           reference_supporting_text  \\\n",
      "0  This edge comes from the TRED dataset in \"tfta...   \n",
      "\n",
      "                                  reference_uri subject_id  \n",
      "0  https://www.ncbi.nlm.nih.gov/pubmed/17202159  HGNC:8615  \n",
      "\n",
      "The regulation network edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/regulation_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodes()\" is running...\n",
      "\n",
      "* Total number of nodes: 16971\n",
      "\n",
      "* Trap genes without gene symbol, i.e. genes with discontinued entrez ID...\n",
      "* Number of concepts without gene symbol: 140\n",
      "* Check that all genes without gene symbol are identified by entrez ID...\n",
      "* Number of concepts without gene symbol by namespace:  NCBIGene    140\n",
      "Name: id, dtype: int64\n",
      "\n",
      "* Querying BioThings to map retired Entrez to gene symbols...\n",
      "querying 1-140...done.\n",
      "Finished.\n",
      "89 input query terms found no hit:\n",
      "\t['79907', '79911', '93333', '121301', '143902', '146856', '151720', '197379', '219392', '221943', '2\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "* Querying BioThings to retrieve node attributes...\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "querying 10001-11000...done.\n",
      "querying 11001-12000...done.\n",
      "querying 12001-13000...done.\n",
      "querying 13001-14000...done.\n",
      "querying 14001-15000...done.\n",
      "querying 15001-16000...done.\n",
      "querying 16001-16847...done.\n",
      "Finished.\n",
      "53 input query terms found no hit:\n",
      "\t['CACBINDINGPROTEIN', 'TAL1BETAITF2', 'E2F1DP1RB', 'COREBINDINGFACTOR', 'CREBP1', 'ISRE', 'T3R', 'TC\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "* This is the size of the nodes file data structure: (16971, 6)\n",
      "* These are the nodes attributes: Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "                                         description         id          name  \\\n",
      "0  This gene is a member of the paired box (PAX) ...  HGNC:8615  paired box 1   \n",
      "\n",
      "  preflabel semantic_groups    synonyms  \n",
      "0      PAX1            GENE  HUP48|OFC2  \n",
      "\n",
      "The regulation network nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/regulation_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "CPU times: user 2min 19s, sys: 529 ms, total: 2min 19s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare msigdb data\n",
    "gmt_path = './regulation/msigdb/data/c3.tft.v6.1.entrez.gmt'\n",
    "regulation.prepare_msigdb_data(gmt_path)\n",
    "\n",
    "# prepare individual networks\n",
    "data = regulation.load_tf_gene_edges()\n",
    "dicts = regulation.get_gene_id_normalization_dictionaries(data)\n",
    "data_edges = regulation.prepare_data_edges(data, dicts)\n",
    "\n",
    "# prepare regulation network\n",
    "reg_network = regulation.prepare_regulation_edges(data_edges)\n",
    "\n",
    "# build network with graph schema\n",
    "reg_edges = regulation.build_edges(reg_network)\n",
    "reg_nodes = regulation.build_nodes(reg_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regulation network is returned as both digital object (`reg_edges`, `reg_nodes`) and CSV files at _**graph/**_ (`regulation_edges_version.csv`, `regulation_nodes_version.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'list'>\n",
      "type nodes: <class 'list'>\n",
      "\n",
      "len edges: 197267\n",
      "len nodes: 16971\n",
      "\n",
      "attribute edges: dict_keys(['subject_id', 'object_id', 'property_id', 'property_label', 'property_description', 'property_uri', 'reference_uri', 'reference_supporting_text', 'reference_date'])\n",
      "attribute nodes: dict_keys(['id', 'semantic_groups', 'preflabel', 'name', 'synonyms', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(reg_edges))\n",
    "print('type nodes:', type(reg_nodes))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(reg_edges))\n",
    "print('len nodes:', len(reg_nodes))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', reg_edges[0].keys())\n",
    "print('attribute nodes:', reg_nodes[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CURATED NETWORK\n",
    "#### import curation\n",
    "\n",
    "We retrieved and prepared curated edges using the `curation` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"read_network()\" is running...\n",
      "\n",
      "Reading and concatenating all curated statements in the network...\n",
      "\n",
      "* Curation edge files concatenated shape: (322, 22)\n",
      "\n",
      "Reading and concatenating all curated nodes in the network...\n",
      "\n",
      "* Curation node files concatenated shape: (318, 9)\n",
      "\n",
      "Finished read_network().\n",
      "\n",
      "\n",
      "The function \"prepare_data_edges()\" is running...\n",
      "\n",
      "Preparing curated network...\n",
      "\n",
      "Saving curated network at curation/...\n",
      "\n",
      "*Curated edges data structure shape: (321, 9)\n",
      "*Curated edges data structure fields: Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "The curated edges are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/curation/curated_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished prepare_data_edges().\n",
      "\n",
      "\n",
      "The function \"prepare_data_nodes()\" is running...\n",
      "\n",
      "Preparing curated nodes...\n",
      "\n",
      "Saving curated nodes at curation/...\n",
      "\n",
      "*Curated nodes data structure shape: (288, 5)\n",
      "*Curated nodes data structure fields: Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'description'], dtype='object')\n",
      "\n",
      "The curated nodes are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/curation/curated_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished prepare_data_nodes().\n",
      "\n",
      "\n",
      "The function \"prepare_curated_edges()\" is running...\n",
      "\n",
      "Preparing curated edges to graph schema...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Adding diseases to MONDO ID network...\n",
      "\n",
      "Adding gene to protein network...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "\n",
      "Drop duplicated gene-protein relations...\n",
      "\n",
      "Finished prepare_curated_edges().\n",
      "\n",
      "\n",
      "The function \"prepare_curated_nodes()\" is running...\n",
      "\n",
      "Preparing curated nodes to graph schema...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "\n",
      "* Querying BioThings to map Entrez gene IDs to HGNC IDs...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Adding diseases described by the MONDO ontology...\n",
      "\n",
      "Adding Name attribute: gene names from BioThings...\n",
      "\n",
      "* Querying BioThings to map gene symbols to name...\n",
      "querying 1-298...done.\n",
      "Finished.\n",
      "38 input query terms found dup hits:\n",
      "\t[('or', 5), ('NGLY1', 3), ('of', 16), ('1', 14), ('by', 9), ('MRS', 9), ('CSF', 8), ('acid', 4), ('B\n",
      "592 input query terms found no hit:\n",
      "\t['NGLY1-deficiency', 'misfolded', 'incompletely', 'synthesized', 'protein', 'catabolic', 'process', \n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "Preparing encoding genes from ngly1 curated network...\n",
      "\n",
      "Adding BioThings annotation: gene symbol, name, synonyms, description...\n",
      "\n",
      "* Querying BioThings to map UniProt IDs to HGNC IDs, gene symbol, name, aliases, and description...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "\n",
      "Finished prepare_curated_nodes().\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "\n",
      "Save curated graph edges file at graph/...\n",
      "\n",
      "* This is the size of the edges file data structure: (362, 10)\n",
      "* These are the edges attributes: Index(['g2p_mark', 'object_id', 'property_description', 'property_id',\n",
      "       'property_label', 'property_uri', 'reference_date',\n",
      "       'reference_supporting_text', 'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "  g2p_mark     object_id property_description property_id property_label  \\\n",
      "0        0  DOID:0060728                  NaN  RO:0002200  has phenotype   \n",
      "\n",
      "  property_uri reference_date  \\\n",
      "0          NaN     2016-07-07   \n",
      "\n",
      "                           reference_supporting_text  \\\n",
      "0  NGLY1 deficiency (OMIM 610661 and 615273), or ...   \n",
      "\n",
      "                                  reference_uri  subject_id  \n",
      "0  https://www.ncbi.nlm.nih.gov/pubmed/27388694  HGNC:17646  \n",
      "\n",
      "The curation network edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/curated_graph_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodges()\" is running...\n",
      "\n",
      "Save curated graph nodes file at graph/...\n",
      "\n",
      "* This is the size of the nodes file data structure: (302, 6)\n",
      "* These are the nodes attributes: Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "     description            id name         preflabel semantic_groups  \\\n",
      "0  Human disease  DOID:0060728  NaN  NGLY1-deficiency            DISO   \n",
      "\n",
      "                                            synonyms  \n",
      "0  congenital disorder of deglycosylation|congeni...  \n",
      "\n",
      "The curation network nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/curated_graph_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "CPU times: user 2.75 s, sys: 629 ms, total: 3.38 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# graph v3.2\n",
    "# read network from drive and concat all curated statements\n",
    "curation_edges, curation_nodes = curation.read_network(version='v20180118')\n",
    "\n",
    "# prepare data edges and nodes\n",
    "data_edges = curation.prepare_data_edges(curation_edges)\n",
    "data_nodes = curation.prepare_data_nodes(curation_nodes)\n",
    "\n",
    "# prepare curated edges and nodes\n",
    "curated_network = curation.prepare_curated_edges(data_edges)\n",
    "curated_concepts = curation.prepare_curated_nodes(data_nodes)\n",
    "\n",
    "\n",
    "# build edges and nodes files\n",
    "curation_edges = curation.build_edges(curated_network)\n",
    "curation_nodes = curation.build_nodes(curated_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Curated network is returned as both digital object (`curation_edges`, `curation_nodes`) and CSV files at _**graph/**_ (`curated_graph_edges_version.csv`, `curated_graph_nodes_version.csv`)\n",
    "- The original curated network, i.e. without graph data model normalization, is saved as CSV files at _**curation/**_ (`curated_edges_version.csv`, `curated_nodes_version.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'list'>\n",
      "type nodes: <class 'list'>\n",
      "\n",
      "len edges: 362\n",
      "len nodes: 302\n",
      "\n",
      "attribute edges: dict_keys(['subject_id', 'object_id', 'property_id', 'property_label', 'property_description', 'property_uri', 'reference_uri', 'reference_supporting_text', 'reference_date', 'g2p_mark'])\n",
      "attribute nodes: dict_keys(['id', 'semantic_groups', 'preflabel', 'name', 'synonyms', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(curation_edges))\n",
    "print('type nodes:', type(curation_nodes))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(curation_edges))\n",
    "print('len nodes:', len(curation_nodes))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', curation_edges[0].keys())\n",
    "print('attribute nodes:', curation_nodes[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MONARCH NETWORK\n",
    "#### import monarch\n",
    "We retrieved edges from Monarch using the `monarch` module.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- From 8 seed nodes we retrieved 1st shell nodes\n",
    "- From all seed and 1st shell nodes we retrieved ortho-phenotypes\n",
    "- We retrieved extra edges among all of them, i.e. extra connectivity between: seed, 1st shell, ortholog-phenotype nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"get_neighbours_list()\" is running. Its runtime may take some minutes. If you interrupt the process, you will lose all the nodes retrieved and you should start over the execution of this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 8/8 [00:16<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished get_neighbours_list().\n",
      "\n",
      "774\n",
      "\n",
      "The function \"get_orthopheno_list()\" is running. Its runtime may take some hours. If you interrupt the process, you will lose all the nodes retrieved and you should start over the execution of this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 8/8 [00:09<00:00,  1.20s/it]\n",
      "100%|ââââââââââ| 109/109 [01:53<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished get_orthopheno_list().\n",
      "\n",
      "366\n",
      "\n",
      "The function \"get_orthopheno_list()\" is running. Its runtime may take some hours. If you interrupt the process, you will lose all the nodes retrieved and you should start over the execution of this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 774/774 [32:25<00:00,  3.42s/it]  \n",
      "100%|ââââââââââ| 1680/1680 [28:15<00:00,  1.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished get_orthopheno_list().\n",
      "\n",
      "4518\n",
      "genelist:  5666\n",
      "\n",
      "The function \"extract_edges()\" is running. Its runtime may take some hours. If you interrupt the process, you will lose all the edges retrieved and you should start over the execution of this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 5291/5291 [2:10:53<00:00,  2.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished extract_edges(). To save the retrieved Monarch edges use the function \"print_network()\".\n",
      "\n",
      "network:  51825\n",
      "\n",
      "Saving Monarch edges at: '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_connections_v2019-06-17.csv'...\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "df (51825, 9)\n",
      "\n",
      "* This is the size of the edges file data structure: (51825, 9)\n",
      "* These are the edges attributes: Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "                   object_id property_description    property_id  \\\n",
      "0  ZFIN:ZDB-GENE-030131-2415                   NA  RO:HOM0000020   \n",
      "\n",
      "                          property_label  \\\n",
      "0  in 1 to 1 orthology relationship with   \n",
      "\n",
      "                                   property_uri reference_date  \\\n",
      "0  http://purl.obolibrary.org/obo/RO_HOM0000020             NA   \n",
      "\n",
      "                           reference_supporting_text reference_uri  \\\n",
      "0  This edge comes from the Monarch Knowledge Gra...            NA   \n",
      "\n",
      "           subject_id  \n",
      "0  NCBIGene:100561047  \n",
      "\n",
      "The Monarch network edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodes()\" is running...\n",
      "Number of concepts: 5291\n",
      "Number of nodes CURIEs: 29\n",
      "List of nodes CURIEs: dict_keys(['NCBIGene', 'ZFIN', 'RGD', 'GO', 'MGI', 'HGNC', 'WormBase', 'MP', 'HP', 'UBERON', 'SGD', 'Xenbase', 'WBPhenotype', 'FlyBase', 'FBbt', 'MONDO', 'ClinVarVariant', 'CL', 'ZP', 'BNODE', 'KEGG-path', 'REACT', 'Coriell', 'MONARCH', 'FBcv', 'APO', 'ENSEMBL', 'OMIM', 'MMRRC'])\n",
      "\n",
      "Adding BioThings annotation: gene name, synonyms, description...\n",
      "symbols: 1938\n",
      "querying 1-1000...done.\n",
      "querying 1001-1938...done.\n",
      "Finished.\n",
      "322 input query terms found dup hits:\n",
      "\t[('cav1', 3), ('Stat5a', 2), ('Ufd1', 2), ('UFD1', 9), ('RAD23B', 7), ('NRF1', 8), ('PSMC1', 7), ('B\n",
      "56 input query terms found no hit:\n",
      "\t['Aqp1<tm1Ask>/Aqp1<tm1Ask>', '[involves:', 'C57BL/6J', '*', 'CD-1]', 'Y50D4B.3', 'si:dkey-242e21.3'\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "* This is the size of the nodes file data structure: (5291, 6)\n",
      "* These are the nodes attributes: Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "                                         description                  id  \\\n",
      "0  The scaffolding protein encoded by this gene i...  NCBIGene:100561047   \n",
      "\n",
      "         name preflabel semantic_groups                             synonyms  \n",
      "0  caveolin 1      cav1            GENE  BSCL3|CGL3|LCCNS|MSTP085|PPH3|VIP21  \n",
      "\n",
      "The Monarch network nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "CPU times: user 9min 34s, sys: 42.1 s, total: 10min 16s\n",
      "Wall time: 3h 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare data to graph schema\n",
    "# seed nodes\n",
    "seedList = [ \n",
    "    'MONDO:0014109', # NGLY1 deficiency\n",
    "    'HGNC:17646', # NGLY1 human gene\n",
    "    'HGNC:633', # AQP1 human gene\n",
    "    'MGI:103201', # AQP1 mouse gene\n",
    "    'HGNC:7781', # NFE2L1 human gene\n",
    "    'HGNC:24622', # ENGASE human gene\n",
    "    'HGNC:636', # AQP3 human gene\n",
    "    'HGNC:19940' # AQP11 human gene\n",
    "] \n",
    "\n",
    "# get first shell of neighbours\n",
    "neighboursList = monarch.get_neighbours_list(seedList)\n",
    "print(len(neighboursList))\n",
    "\n",
    "# introduce animal model ortho-phenotypes for seed and 1st shell neighbors\n",
    "## For seed nodes:\n",
    "seed_orthophenoList = monarch.get_orthopheno_list(seedList)\n",
    "print(len(seed_orthophenoList))\n",
    "## For 1st shell nodes:\n",
    "neighbours_orthophenoList = monarch.get_orthopheno_list(neighboursList)\n",
    "print(len(neighbours_orthophenoList))\n",
    "\n",
    "# network nodes: seed + 1shell + ortholog-phentoype\n",
    "geneList = sum([seedList,\n",
    "                neighboursList,\n",
    "                seed_orthophenoList,\n",
    "                neighbours_orthophenoList], \n",
    "               [])\n",
    "print('genelist: ',len(geneList))\n",
    "\n",
    "# get Monarch network\n",
    "monarch_network = monarch.extract_edges(geneList)\n",
    "print('network: ',len(monarch_network))\n",
    "\n",
    "# save edges\n",
    "monarch.print_network(monarch_network, 'monarch_connections')\n",
    "\n",
    "# build network with graph schema \n",
    "monarch_edges = monarch.build_edges(monarch_network)\n",
    "monarch_nodes = monarch.build_nodes(monarch_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'list'>\n",
      "type nodes: <class 'list'>\n",
      "\n",
      "len edges: 51825\n",
      "len nodes: 5291\n",
      "\n",
      "attribute edges: dict_keys(['subject_id', 'object_id', 'property_id', 'property_label', 'property_description', 'property_uri', 'reference_uri', 'reference_supporting_text', 'reference_date'])\n",
      "attribute nodes: dict_keys(['id', 'semantic_groups', 'preflabel', 'name', 'synonyms', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(monarch_edges))\n",
    "print('type nodes:', type(monarch_nodes))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(monarch_edges))\n",
    "print('len nodes:', len(monarch_nodes))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', monarch_edges[0].keys())\n",
    "print('attribute nodes:', monarch_nodes[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monarch network is returned as both digital object (`monarch_edges`, `monarch_nodes`) and CSV files at _**monarch/**_ (`monarch_edges_version.csv`, `monarch_nodes_version.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph library\n",
    "### Create the review knowledge graph\n",
    "#### import graph\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Load Networks and calculate graph nodes\n",
    "* Retrieve extra connectivity for the graph from Monarch\n",
    "* Build the review graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"graph_nodes()\" is running...\n",
      "\n",
      "Preparing networks...\n",
      "Curated:\n",
      "(362, 10)\n",
      "Index(['g2p_mark', 'object_id', 'property_description', 'property_id',\n",
      "       'property_label', 'property_uri', 'reference_date',\n",
      "       'reference_supporting_text', 'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Monarch:\n",
      "(51825, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Transcriptomics:\n",
      "(386, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Regulatory:\n",
      "(197267, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Concatenating into a graph...\n",
      "(52573, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(52573, 9)\n",
      "\n",
      "Merging tf-gene network to the graph...\n",
      "(9798, 9)\n",
      "\n",
      "Saving tf merged edges...\n",
      "\n",
      "The regulation graph merged edges are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/regulation_graph_edges_v2019-06-17.csv\n",
      "\n",
      "(62371, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(62371, 9)\n",
      "\n",
      "Generating graph nodes...\n",
      "(9996, 1)\n",
      "\n",
      "Finished graph_nodes().\n",
      "\n",
      "\n",
      "The function \"extract_edges()\" is running. Its runtime may take some hours. If you interrupt the process, you will lose all the edges retrieved and you should start over the execution of this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 9996/9996 [5:25:52<00:00,  2.65s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished extract_edges(). To save the retrieved Monarch edges use the function \"print_network()\".\n",
      "\n",
      "network:  273533\n",
      "\n",
      "Saving Monarch edges at: '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_connections_graph_v2019-06-17.csv'...\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "df (273533, 9)\n",
      "\n",
      "* This is the size of the edges file data structure: (273533, 9)\n",
      "* These are the edges attributes: Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "    object_id property_description    property_id  \\\n",
      "0  HGNC:11011                   NA  RO:HOM0000017   \n",
      "\n",
      "                   property_label  \\\n",
      "0  in orthology relationship with   \n",
      "\n",
      "                                   property_uri reference_date  \\\n",
      "0  http://purl.obolibrary.org/obo/RO_HOM0000017             NA   \n",
      "\n",
      "                           reference_supporting_text reference_uri  \\\n",
      "0  This edge comes from the Monarch Knowledge Gra...            NA   \n",
      "\n",
      "            subject_id  \n",
      "0  FlyBase:FBgn0034045  \n",
      "\n",
      "The Monarch network edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodes()\" is running...\n",
      "Number of concepts: 9689\n",
      "Number of nodes CURIEs: 29\n",
      "List of nodes CURIEs: dict_keys(['FlyBase', 'HGNC', 'CL', 'UBERON', 'GO', 'ZFIN', 'MGI', 'NCBIGene', 'WormBase', 'WBPhenotype', 'HP', 'REACT', 'ClinVarVariant', 'FBbt', 'ZP', 'RGD', 'MP', 'FBcv', 'Xenbase', 'SGD', 'MONARCH', 'BNODE', 'MONDO', 'APO', 'KEGG-path', 'ENSEMBL', 'OMIM', 'Coriell', 'MMRRC'])\n",
      "\n",
      "Adding BioThings annotation: gene name, synonyms, description...\n",
      "symbols: 6261\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-6261...done.\n",
      "Finished.\n",
      "325 input query terms found dup hits:\n",
      "\t[('RGS20', 7), ('FASN', 2), ('FBXW7', 6), ('Ubqln1', 2), ('ubqln1', 2), ('GSK3B', 7), ('RPS27A', 5),\n",
      "69 input query terms found no hit:\n",
      "\t['(human)', '(human)', '(human)', '(human)', '(human)', '(human)', '(human)', 'Y50D4B.3', 'E7F5X7', \n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "\n",
      "* This is the size of the nodes file data structure: (9689, 6)\n",
      "* These are the nodes attributes: Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "  description                   id                     name preflabel  \\\n",
      "0          NA  FlyBase:FBgn0034045  uncharacterized protein    CG8249   \n",
      "\n",
      "  semantic_groups     synonyms  \n",
      "0            GENE  Dmel\\CG8249  \n",
      "\n",
      "The Monarch network nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/monarch/monarch_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "\n",
      "The function \"build_edges()\" is running...\n",
      "\n",
      "Preparing networks...\n",
      "Curated:\n",
      "(362, 10)\n",
      "Index(['g2p_mark', 'object_id', 'property_description', 'property_id',\n",
      "       'property_label', 'property_uri', 'reference_date',\n",
      "       'reference_supporting_text', 'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Monarch:\n",
      "(273533, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Transcriptomics:\n",
      "(386, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "Regulatory:\n",
      "(9798, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "Concatenating into a graph...\n",
      "(284079, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(284079, 9)\n",
      "\n",
      "Saving final graph...\n",
      "(284079, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "* This is the size of the edges file data structure: (284079, 9)\n",
      "* These are the edges attributes: Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "   subject_id property_id     object_id  \\\n",
      "0  HGNC:17646  RO:0002200  DOID:0060728   \n",
      "\n",
      "                                  reference_uri  \\\n",
      "0  https://www.ncbi.nlm.nih.gov/pubmed/27388694   \n",
      "\n",
      "                           reference_supporting_text reference_date  \\\n",
      "0  NGLY1 deficiency (OMIM 610661 and 615273), or ...     2016-07-07   \n",
      "\n",
      "  property_label property_description  \\\n",
      "0  has phenotype                  NaN   \n",
      "\n",
      "                                property_uri  \n",
      "0  http://purl.obolibrary.org/obo/RO_0002200  \n",
      "\n",
      "The NGLY1 Deficiency knowledge graph edges are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/graph_edges_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_edges().\n",
      "\n",
      "\n",
      "The function \"build_nodes()\" is running...\n",
      "\n",
      "Preparing networks...\n",
      "Curated:\n",
      "(302, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "Monarch:\n",
      "(9689, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "Transcriptomics:\n",
      "(386, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "Regulatory:\n",
      "(16971, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "\n",
      "Annotating nodes in the graph...\n",
      "graph from e (9996, 1)\n",
      "annotation check\n",
      "curated (301, 6)\n",
      "monarch (9689, 6)\n",
      "rna (386, 6)\n",
      "regulation (4242, 6)\n",
      "\n",
      "Concatenating all nodes...\n",
      "graph ann (14618, 6)\n",
      "diff set()\n",
      "\n",
      "Drop duplicated rows...\n",
      "(11763, 6)\n",
      "\n",
      "Drop duplicated nodes...\n",
      "(9996, 6)\n",
      "\n",
      "All graph nodes are annotated.\n",
      "Regulation nodes not in the graph: 12729\n",
      "\n",
      "Saving final graph...\n",
      "(9996, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "\n",
      "* This is the size of the edges file data structure: (9996, 6)\n",
      "* These are the edges attributes: Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "* This is the first record:\n",
      "             id semantic_groups         preflabel  \\\n",
      "0  DOID:0060728            DISO  NGLY1-deficiency   \n",
      "\n",
      "                                            synonyms name    description  \n",
      "0  congenital disorder of deglycosylation|congeni...  NaN  Human disease  \n",
      "\n",
      "The NGLY1 Deficiency knowledge graph nodes are built and saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/graph/graph_nodes_v2019-06-17.csv\n",
      "\n",
      "\n",
      "Finished build_nodes().\n",
      "\n",
      "CPU times: user 13min 52s, sys: 51.8 s, total: 14min 44s\n",
      "Wall time: 5h 27min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load networks and calculate graph nodes\n",
    "graph_nodes_list, reg_graph_edges = graph.graph_nodes(\n",
    "    curation=curation_edges,\n",
    "    monarch=monarch_edges,\n",
    "    transcriptomics=rna_edges,\n",
    "    regulation=reg_edges\n",
    ")\n",
    "\n",
    "# Monarch graph connectivity\n",
    "## get Monarch edges\n",
    "monarch_network_graph = monarch.extract_edges(graph_nodes_list)\n",
    "print('network: ',len(monarch_network_graph))\n",
    "\n",
    "## save Monarch network\n",
    "monarch.print_network(monarch_network_graph, 'monarch_connections_graph')\n",
    "\n",
    "## build Monarch network with graph schema\n",
    "monarch_graph_edges = monarch.build_edges(monarch_network_graph)\n",
    "monarch_graph_nodes = monarch.build_nodes(monarch_network_graph)\n",
    "\n",
    "# build review graph\n",
    "edges = graph.build_edges(\n",
    "    curation=curation_edges,\n",
    "    monarch=monarch_graph_edges,\n",
    "    transcriptomics=rna_edges,\n",
    "    regulation=reg_graph_edges\n",
    ")\n",
    "nodes = graph.build_nodes(\n",
    "    statements=edges,\n",
    "    curation=curation_nodes,\n",
    "    monarch=monarch_graph_nodes,\n",
    "    transcriptomics=rna_nodes,\n",
    "    regulation=reg_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regulation edges _merged_ with the graph is returned as both digital object (`reg_graph_edges`) and CSV file at _**graph/**_ (`regulation_graph_edges_version.csv`)\n",
    "- Monarch network is returned as both digital object (`monarch_graph_edges`, `monarch_graph_nodes`) and CSV files at _**monarch/**_ (`monarch_edges_version.csv`, `monarch_nodes_version.csv`) overwritten the previous one.\n",
    "- Review knowledge graph is returned as both digital object (`edges`, `nodes`) and CSV files at _**graph/**_ (`graph_edges_version.csv`, `graph_nodes_version.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'pandas.core.frame.DataFrame'>\n",
      "type nodes: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "len edges: 284079\n",
      "len nodes: 9996\n",
      "\n",
      "attribute edges: Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "attribute nodes: Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(edges))\n",
    "print('type nodes:', type(nodes))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(edges))\n",
    "print('len nodes:', len(nodes))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', edges.columns)\n",
    "print('attribute nodes:', nodes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4jlib library\n",
    "### Import the graph into Neo4j graph database\n",
    "#### import neo4jlib\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Create Neo4j server instance\n",
    "- Import review graph into the Neo4j graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Neo4j community v3.5.6 server instance...\n",
      "Downloading the server from neo4j.org...\n",
      "Preparing the server...\n",
      "Configuration adjusted!\n",
      "Starting the server...\n",
      "Neo4j v3.5.6 is running.\n",
      "The name of the neo4j directory is neo4j-community-3.5.6\n",
      "statements:  284079\n",
      "concepts:  9996\n",
      "\n",
      "File './neo4j-community-3.5.6/import/ngly1/ngly1_statements.csv' saved.\n",
      "\n",
      "File './neo4j-community-3.5.6/import/ngly1/ngly1_concepts.csv' saved.\n",
      "\n",
      "The function \"do_import()\" is running...\n",
      "\n",
      "The graph is imported into the server. Neo4j is running.You can start exploring and querying for hypothesis. If you change ports or authentication in the Neo4j configuration file, the hypothesis-generation modules performance, hypothesis.py and summary.py, will be affected.\n",
      "\n",
      "CPU times: user 4.4 s, sys: 344 ms, total: 4.75 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a Neo4j server instance\n",
    "neo4j_dir = neo4jlib.create_neo4j_instance()\n",
    "print('The name of the neo4j directory is {}'.format(neo4j_dir))\n",
    "\n",
    "# import to graph database\n",
    "## prepare the graph to neo4j format\n",
    "edges_df = utils.get_dataframe(edges)\n",
    "nodes_df = utils.get_dataframe(nodes)\n",
    "statements = neo4jlib.get_statements(edges_df)\n",
    "concepts = neo4jlib.get_concepts(nodes_df)\n",
    "print('statements: ', len(statements))\n",
    "print('concepts: ',len(concepts))\n",
    "\n",
    "## save files into neo4j import dir\n",
    "neo4j_path = './{}'.format(neo4j_dir)\n",
    "neo4jlib.save_neo4j_files(statements, neo4j_path, file_type = 'statements')\n",
    "neo4jlib.save_neo4j_files(concepts, neo4j_path, file_type = 'concepts')\n",
    "\n",
    "## import graph into neo4j database\n",
    "neo4jlib.do_import(neo4j_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type edges: <class 'pandas.core.frame.DataFrame'>\n",
      "type nodes: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "len edges: 284079\n",
      "len nodes: 9996\n",
      "\n",
      "attribute edges: Index([':START_ID', ':TYPE', ':END_ID', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description:IGNORE', 'property_uri'],\n",
      "      dtype='object')\n",
      "attribute nodes: Index(['id:ID', ':LABEL', 'preflabel', 'synonyms:IGNORE', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print type of objects\n",
    "print('type edges:', type(statements))\n",
    "print('type nodes:', type(concepts))\n",
    "print()\n",
    "\n",
    "# print objects sizes\n",
    "print('len edges:', len(statements))\n",
    "print('len nodes:', len(concepts))\n",
    "print()\n",
    "\n",
    "# print object attribute\n",
    "print('attribute edges:', statements.columns)\n",
    "print('attribute nodes:', concepts.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hypothesis-generation library\n",
    "### Query the graph for mechanistic explanation, then summarize the extracted paths\n",
    "#### import hypothesis, summary\n",
    "\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Retrieve orthopheno paths with the `query` method.\n",
    "* Retrieve orthopheno paths using relaxing node degree parameters with the `query` method.\n",
    "* Retrieve orthopheno paths from a more open query topology with the `open_query` method.\n",
    "* Get hypothesis summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ortopheno query with general nodes/relations removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"query()\" is running...\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_pwdl50_phdl20_paths_v2019-06-17.json\n",
      "\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_pwdl50_phdl20_paths_v2019-06-17.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis generator has finished. 2 QUERIES completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 643 ms, sys: 19.5 ms, total: 663 ms\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get orthopheno paths\n",
    "seed = list([\n",
    "        'HGNC:17646',  # NGLY1 human gene\n",
    "        'HGNC:633'  # AQP1 human gene\n",
    "])\n",
    "hypothesis.query(seed,queryname='ngly1_aqp1',port='7687') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"query()\" is running...\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_pwdl1000_phdl1000_paths_v2019-06-17.json\n",
      "\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_pwdl1000_phdl1000_paths_v2019-06-17.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis generator has finished. 2 QUERIES completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 60 ms, total: 4.15 s\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get orthopheno paths relaxing pathway and phenotype node degrees\n",
    "seed = list([\n",
    "        'HGNC:17646',  # NGLY1 human gene\n",
    "        'HGNC:633'  # AQP1 human gene\n",
    "])\n",
    "hypothesis.query(seed, queryname='ngly1_aqp1', pwdegree='1000', phdegree='1000', port='7687')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"open_query()\" is running...\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_paths_v2019-06-17.json\n",
      "\n",
      "\n",
      "The query results are saved at: /home/nuria/workspace/graph-hypothesis-generation-lib/plan/hypothesis/query_ngly1_aqp1_paths_v2019-06-17.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis generator has finished. 2 QUERIES completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 180 ms, total: 12.8 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get orthopheno paths from a more open query topogology\n",
    "seed = list([\n",
    "        'HGNC:17646',  # NGLY1 human gene\n",
    "        'HGNC:633'  # AQP1 human gene\n",
    "])\n",
    "hypothesis.open_query(seed,queryname='ngly1_aqp1',port='7687')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function \"query_parser()\" is running...\n",
      "\n",
      "Finished query_parser().\n",
      "\n",
      "\n",
      "The function \"query_parser()\" is running...\n",
      "\n",
      "Finished query_parser().\n",
      "\n",
      "\n",
      "The function \"metapaths()\" is running...\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_metapaths_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_entities_in_metapaths_v2019-06-17.csv' saved.\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_metapaths_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_entities_in_metapaths_v2019-06-17.csv' saved.\n",
      "\n",
      "Finished metapaths().\n",
      "\n",
      "\n",
      "The function \"nodes()\" is running...\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_nodes_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_nodes_v2019-06-17.csv' saved.\n",
      "\n",
      "Finished nodes().\n",
      "\n",
      "\n",
      "The function \"node_types()\" is running...\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_node_types_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_node_types_v2019-06-17.csv' saved.\n",
      "\n",
      "Finished node_types().\n",
      "\n",
      "\n",
      "The function \"edges()\" is running...\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_edges_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_edges_v2019-06-17.csv' saved.\n",
      "\n",
      "Finished edges().\n",
      "\n",
      "\n",
      "The function \"edge_types()\" is running...\n",
      "\n",
      "Printing summaries...\n",
      "\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:17646_target:HGNC:633_summary_edge_types_v2019-06-17.csv' saved.\n",
      "\n",
      "File '/home/nuria/workspace/graph-hypothesis-generation-lib/plan/summaries/query_source:HGNC:633_target:HGNC:17646_summary_edge_types_v2019-06-17.csv' saved.\n",
      "\n",
      "Finished edge_types().\n",
      "\n",
      "CPU times: user 3min 32s, sys: 130 ms, total: 3min 32s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get summary\n",
    "data = summary.path_load('./hypothesis/query_ngly1_aqp1_paths_v2019-06-17.json')\n",
    "\n",
    "# parse data for summarization\n",
    "data_parsed = list()\n",
    "for query in data:\n",
    "    query_parsed = summary.query_parser(query)\n",
    "    data_parsed.append(query_parsed)\n",
    "summary.metapaths(data_parsed)\n",
    "summary.nodes(data_parsed)\n",
    "summary.node_types(data_parsed)\n",
    "summary.edges(data_parsed)\n",
    "summary.edge_types(data_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
